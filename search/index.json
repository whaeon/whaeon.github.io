[{"content":"默认过期时间 采用 kubeadm 方式安装集群，集群中部分默认证书为一年，一年过期后，会导致api service不可用，使用过程中会出现：x509: certificate has expired or is not yet valid.\n证书还在可用期内，k8s集群正常访问 方案一：启用自动轮换 kubelet 证书 kubelet证书分为server和client两种， k8s 1.9默认启用了client证书的自动轮换，但server证书自动轮换需要用户开启\n 增加 kubelet 参数  1 2  # 在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf 增加如下参数 Environment=\u0026#34;KUBELET_EXTRA_ARGS=--feature-gates=RotateKubeletServerCertificate=true\u0026#34;    增加 controller-manager 参数  1 2 3 4 5 6  # 在/etc/kubernetes/manifests/kube-controller-manager.yaml 添加如下参数 - command: - kube-controller-manager - --experimental-cluster-signing-duration=87600h0m0s - --feature-gates=RotateKubeletServerCertificate=true - ....    创建 rbac 对象 创建rbac对象，允许节点轮换kubelet server证书：   rbac offical docs\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  cat \u0026gt; ca-update.yaml \u0026lt;\u0026lt; EOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver rules: - apiGroups: - certificates.k8s.io resources: - certificatesigningrequests/selfnodeserver verbs: - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubeadm:node-autoapprove-certificate-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:nodes EOF kubectl create –f ca-update.yaml   方案二：修改源代码以调整证书过期时间 该步骤比较繁琐，需要自己重新编译官方镜像，同时修改源码。 因此，建议转至原文查看：here\n证书已过期，集群的 apiserver 已无法启动 重新签发证书，但该方式仅针对 k8s 1.14.x 及以上版本 1.14.x 及以下版本，可参考这里配置\n 准备 kubeadm.conf 配置文件  1 2 3 4 5 6 7  apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: v1.14.1 #--\u0026gt;这里改成你集群对应的版本 imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers # 这里使用国内的镜像仓库，未使用国内镜像仓库，可不配置该参数。 # 重新签发的时候可能会报错：could not fetch a Kubernetes version from the internet: unable to get URL \u0026#34;https://dl.k8s.io/release/stable-1.txt\u0026#34;    重新签发  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  kubeadm certs renew all --config=/root/kubeadm.conf 运行如上命令会重新生成以下证书 #-- /etc/kubernetes/pki/apiserver.key #-- /etc/kubernetes/pki/apiserver.crt #-- /etc/kubernetes/pki/apiserver-etcd-client.key #-- /etc/kubernetes/pki/apiserver-etcd-client.crt #-- /etc/kubernetes/pki/apiserver-kubelet-client.key #-- /etc/kubernetes/pki/apiserver-kubelet-client.crt #-- /etc/kubernetes/pki/front-proxy-client.key #-- /etc/kubernetes/pki/front-proxy-client.crt #-- /etc/kubernetes/pki/etcd/healthcheck-client.key #-- /etc/kubernetes/pki/etcd/healthcheck-client.crt #-- /etc/kubernetes/pki/etcd/peer.key #-- /etc/kubernetes/pki/etcd/peer.crt #-- /etc/kubernetes/pki/etcd/server.key #-- /etc/kubernetes/pki/etcd/server.crt   重新启动 kube-apiserver/kube-scheduler/kube-controller/etcd 这四个容器 如果有多台 master，将第一台生成的相关证书拷贝至其他 master 即可。\n  替换证书更新后的 kubeconfig\n  ","date":"2022-06-15T17:00:06+08:00","permalink":"https://whaeon.github.io/p/kubernetes-blog10/","title":"K8s 集群证书过期时间调整"},{"content":"k8s 集群架构升级 环境准备  至少三台用作 master 的主机 master 节点和 worker 节点全网络连接（公网或私网） 已配置完成的 keepalived 环境（用于多 master 集群的负载均衡）   特别注意：：搭建环境中 kubeadm,kubectl,kubelet 以及 kubernetes cluster version 均为 1.21.2。由于版本在命令上可能有所区别，因此，参考时需注意。\n 操作过程概述  整个集群的升级过程：  配置 configmap kubeadm-config (该 configmap 存放在 kube-system namespace 中) 重新配置 cluster 中各组件服务的配置文件中对应的 apiserver ip/domain 上传集群证书，并获取节点访问证书的秘钥 获取加入集群节点的 token other controlPlane 作为 master 加入集群，同时同步 etcd 的数据。    配置 kubeadm-config  在 configmap kubeadm-config 中添加 controlPlaneEndpoint 参数 配置 certSANs（配置该参数的目的是为了重新初始化包含负载均衡ip的 apiserver 的证书和秘钥）  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # 查看是否有 kubeadm-config 这个 configmap 存在 kubectl get configmap -n kube-system kubectl edit cm kubeadm-config -n kube-system -o yaml # 添加 controlPlaneEndpoint 参数 apiVersion: v1 data: ClusterConfiguration: | apiServer: certSANs: # 添加 certSANs，添加 load_balance_ip/master_node_ip/worker_node_ip/domains。 - ip/domain - ip/domain extraArgs: authorization-mode: Node,RBAC timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 controlPlaneEndpoint: ip:port # 添加 load balance ip:port ...    将修改后的 configmap 导出，同时使用该文件生成新的 apiserver key、cert  1  $ kubectl -n kube-system get configmap kubeadm-config -o jsonpath=\u0026#39;{.data.ClusterConfiguration}\u0026#39; \u0026gt; kubeadm.yaml    移动旧的 apiserver 证书，并生成新的 apiserver key  1 2 3 4 5  $ sudo mv /etc/kubernetes/pki/apiserver.{crt,key} ~ # 删除或移动至其他目录，否则 kubeadm 检测到文件存在，则不会创建新的 cert key $ sudo kubeadm init phase certs apiserver --config kubeadm.yaml # 该命令会生成 apiserver.{crt,key} 两个文件。    重新启动 apiserver container，重新加载 集群配置 container 被 kill 后，集群会重新启动一个 apiserver  1 2  $ docker ps | grep apiserver $ docker kill container_id/container_name   验证证书 验证证书是否存在已添加的 certSANs 列表\n1 2  $ openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text # 输出内容中可以看到包含 certSANs 中写入的 ip 地址列表   配置新的 IP/Domain  修改各组件服务对应的 apiserver ip address 修改的文件包含以下几个部分  kubelet.conf scheduler.conf controller-manager.conf .kube/config cluster-info(in kube-public namespce) kube-proxy(in kube-system namespace)   修改并重新启动对应 container  1 2 3 4 5 6 7 8  $ sudo sed -i \u0026#39;s/old_apiserver_ip:port/new_load_balance_ip:port/\u0026#39; /etc/kubernetes/kubelet.conf $ sudo sed -i \u0026#39;s/old_apiserver_ip:port/new_load_balance_ip:port/\u0026#39; /etc/kubernetes/controller-manager.conf $ sudo sed -i \u0026#39;s/old_apiserver_ip:port/new_load_balance_ip:port/\u0026#39; /etc/kubernetes/scheduler.conf $ sudo sed -i \u0026#39;s/old_apiserver_ip:port/new_load_balance_ip:port/\u0026#39; ~/.kube/config # 修改包含旧 apiserver ip 的两个 configmap $ kubectl edit cm cluster-info -n kube-public $ kubectl edit cm kube-proxy -n kube-systen    重新启动 container，验证相关服务 重新启动 container，以替换 container 中使用的 server ip  1 2 3 4 5  $ docker ps $ docker kill controller-manager_container_id scheduler_container_id $ sudo systemctl restart kubelet # 重启 kubelet 服务，更新 server ip   验证 cluster-info 是否更新\n1 2 3 4 5 6  $ kubectl cluster-info Kubernetes control plane is running at https://192.168.3.230:9443 CoreDNS is running at https://192.168.3.230:9443/api/v1/namespaces/kube- system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;.   上传集群证书 上传证书到集群，并获取其他 control Plane 访问集群证书的秘钥\n1 2 3 4 5  $ sudo kubeadm init phase upload-certs --upload-certs # 输出类似以下内容 [upload-certs] Storing the certificates in Secret \u0026#34;kubeadm-certs\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [upload-certs] Using certificate key: 24d6435f37e76458e267f479fac970640b0cbbb07787833e34fb529bdd0288e0   获取节点加入集群的 token 获取节点加入集群的 token，用做其他 control plane 加入集群\n1 2 3 4 5 6  $ kubeadm token list # 该命令可以查看集群的 token 有哪些 $ kubeadm token create --print-join-command # 生成 token(生成的 token 只有 24h 的有效期) 并打印出加入集群节点的命令，输出类似以下内容 $ kubeadm join 192.168.3.230:9443 --token h3gpgk.x4quidbcuc3dxyek --discovery-token-ca-cert-hash sha256:2d1a25b512933cbd24f9b8945ae708b2b24a6e6857a57ff6cb7477e4600946d0   other controlPlane 作为 master 加入集群  other controlPlane join cluster  1 2  # 结合上面获取到的访问证书秘钥和加入集群的命令使用 $ sudo kubeadm join 192.168.3.230:9443 --token h3gpgk.x4quidbcuc3dxyek --discovery-token-ca-cert-hash sha256:2d1a25b512933cbd24f9b8945ae708b2b24a6e6857a57ff6cb7477e4600946d0 --control-plane --certificate-key 24d6435f37e76458e267f479fac970640b0cbbb07787833e34fb529bdd0288e0    同步 master 中 etcd 的配置 由于 control Plane 的3个节点是先后安装的，所以前面两个节点的 etcd 中并不包含其他 etcd 节点的信息。所以需要同步所有控制平面节点的 etcd 集群配置  1 2 3 4  $ cat /etc/kubernetes/manifests/etcd.yaml ... - --initial-cluster=master=https://ip:2380,master2=https://ip:2380,master3=https://ip:2380 ...   参考 Adding a Name to the Kubernetes API Server Certificate Creating Highly Available clusters with kubeadm 如何将单 master 升级为多 master 集群\n","date":"2022-06-15T16:48:31+08:00","permalink":"https://whaeon.github.io/p/kubernetes-blog09/","title":"kubernetes 集群由单 master 集群升级至多 master 集群"},{"content":"生成新的 kubeconfig 集群所用证书在更新后，对应访问集群的用户的 kubeconfig 也需要同步进行更新。 使用 kubeadm 命令可以直接进行更新，但仅会重新生成 /etc/kubernetes/ 下的部分文件。 重新生成后需要将 /etc/kubernetes 内的文件拷贝至 .kube 内进行使用。\n1 2 3 4 5  sudo kubeadm init phase kubeconfig admin sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/confi   Reference Renewing Kubernetes cluster certificates\n","date":"2022-06-15T16:43:09+08:00","permalink":"https://whaeon.github.io/p/kubernetes-blog08/","title":"kubernetes 集群证书更新后，重新生成 kubeconfig"},{"content":"默认范围 k8s 默认的端口范围是:30000~32767，并由 k8s-apiserver 进行管理.因此，想要改变映射的端口范围，就需要修改 apiserver 配置文件 /etc/kubernetes/manifests/kube-apiserver.yaml 中的相关配置。 通过在 apiserver 配置文件中添加参数 --service-node-port-range 来改变端口范围。\n修改范围 kube-apiserver.yaml 配置修改后如下所示：\n 修改端口范围要注意对集群中已有服务的影响\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:v1kind:Podmetadata:annotations:scheduler.alpha.kubernetes.io/critical-pod:\u0026#34;\u0026#34;creationTimestamp:nulllabels:component:kube-apiservertier:control-planename:kube-apiservernamespace:kube-systemspec:containers:- command:- kube-apiserver...- --service-node-port-range=20000-22767...  Reference Kubernetes service node port range\n","date":"2022-06-15T16:39:18+08:00","permalink":"https://whaeon.github.io/p/kubernetes-blog07/","title":"修改 k8s 集群默认端口号"},{"content":"tmux tmux 默认安装后的快捷键是ctrl + b,但由于这俩键距离着实有点远，为了方便使用，我把它换成了ctrl + a\n修改默认前缀 临时修改 1 2 3  tmux set -g prefix C-a tmux unbind C-b tmux bind C-a send-prefix   永久修改 涉及到的配置文件：\n /etc/tmux.conf修改该配置文件，改变所有用户 tmux 的前缀 ~/.tmux.conf修改当前登录用户的前缀  1 2 3 4  set -g prefix C-a unbind C-b bind C-a send-prefix set -g default-terminal \u0026#34;screen-256color\u0026#34;   ","date":"2022-06-15T16:33:54+08:00","permalink":"https://whaeon.github.io/p/memo-blog06/","title":"tmux 默认快捷键修改"},{"content":"ansible 的 set_fact 模块 ansible 的 module 中有一个 set_fact 模块可以用于处理变量的跨 roles 使用。 通过 ansible-doc set_fact 可以查看详细使用说明\nexample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # role 1- name:get tokenbecome:yesansible.builtin.command:cmd:kubeadm token create --print-join-commandregister:joinToken- name:set vars to hostvarsset_fact:token:\u0026#34;{{ joinToken }}\u0026#34;# role 2- name:join k8s clusterbecome:yesvars:joinToken:\u0026#34;{{ hostvars[\u0026#39;master\u0026#39;][\u0026#39;token\u0026#39;] }}\u0026#34;ansible.builtin.command:cmd:\u0026#34;{{ joinToken.stdout }}\u0026#34;creates:/etc/kubernetes/kubelet.conf  Reference How can I persist an ansible variable across ansible roles?\n","date":"2022-06-15T16:27:29+08:00","permalink":"https://whaeon.github.io/p/memo-blog05/","title":"ansible 使用其他 roles 内的变量"},{"content":"error 复现 当我在 k3s 集群的 master 节点执行 helm 命令安装 vault 时，报错产生。 具体信息为：Error: Kubernetes cluster unreachable: Get \u0026quot;http://localhost:8080/version?timeout=32s\u0026quot;: dial tcp 127.0.0.1:8080: connect: connection refused\nTroubleShooting 在经过面向 google 查找之后，找到了一篇博文，介绍了该问题的解决方法。 helm 3 中取消了 tiller 的使用，通过直接与 apiserver 进行通信来完成交互。而 helm 默认读取的配置文件路径 在 .kube/config，而 k3s 的 config 文件存放路径则是 /etc/rancher/k3s/k3s.yaml\n解决方案  直接使用 export 声明 kubeconfig 文件路径，或是 helm 执行命令时加入 kubeconfig 路径 在配置文件 /etc/profile 中声明 kubeconfig 文件的路径，一劳永逸的解决。  1 2 3 4 5 6 7 8 9  # 方案一 export KUBECONFIG=/etc/rancher/k3s/k3s.yaml # or helm install options --kubeconfig /etc/rancher/k3s/k3s.yaml # 方案二 sudo echo \u0026#34;export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\u0026#34; \u0026gt;\u0026gt; /etc/profile source /etc/profile   ","date":"2022-06-15T16:14:11+08:00","permalink":"https://whaeon.github.io/p/helm-blog03/","title":"k3s 集群使用 helm 安装 vault 时出现 Error"},{"content":"问题描述 在使用 VScode push 代码时，出现 error：No anonymous write access，导致无法 push 代码。\n解决方法 解决方法可以参考：here\n1  export VSCODE_GIT_ASKPASS_EXTRA_ARGS=\u0026#34;--ms-enable-electron-run-as-node\u0026#34;   执行完上述命令，在弹出的网页中绑定对应 github 账号，然后重新 push 即可。\n","date":"2022-06-15T16:10:46+08:00","permalink":"https://whaeon.github.io/p/memo-blog04/","title":"VScode error"},{"content":"基本概念 L2TP 第二层隧道协议（Layer Two Tunneling Protocol）是一种虚拟隧道协议，通常用于虚拟专用网。L2TP协议自身不提供加密与可靠性验证的功能，可以和安全协议搭配使用，从而实现数据的加密传输。L2TP 用来创建隧道连接，而 ipsec 协议则用来加密数据。因此， L2TP 协议经常会和 ipsec 协议一同出现。\n构建 vpn server 该文章主要介绍使用 docker 镜像快速搭建 vpn server 服务器，服务器部署参考了:here\n 首先安装 docker 环境，ubuntu 中 docker 的安装可参考: here 安装 docker-compose, 可以参考：here, docker-compose 能够更方便的管理 docker 编写 docker-compose.yaml 文件，可使用以下内容作为参考。 使用 docker-compose up -d 命令快速启动 vpn server，然后使用docker-compose logs -f 查看容器日志，记下日志中的以下内容：Server IP,IPsec PSK,Username,Password  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  # docker-compose.yaml version: \u0026#39;3\u0026#39; services: vpn: image: hwdsl2/ipsec-vpn-server restart: always env_file: - ./vpn.env ports: - \u0026#34;500:500/udp\u0026#34; - \u0026#34;4500:4500/udp\u0026#34; - \u0026#34;1701:1701\u0026#34; privileged: true hostname: ipsec-vpn-server container_name: ipsec-vpn-server # vpn.env # Note: All the variables to this image are optional. # See README for more information. # To use, uncomment and replace with your own values. # Define IPsec PSK, VPN username and password # - DO NOT put \u0026#34;\u0026#34; or \u0026#39;\u0026#39; around values, or add space around = # - DO NOT use these special characters within values: \\ \u0026#34; \u0026#39; VPN_IPSEC_PSK=random_or_online_generate_PSK VPN_USER=yourself_username VPN_PASSWORD=yourself_password # Define additional VPN users # - DO NOT put \u0026#34;\u0026#34; or \u0026#39;\u0026#39; around values, or add space around = # - DO NOT use these special characters within values: \\ \u0026#34; \u0026#39; # - Usernames and passwords must be separated by spaces # VPN_ADDL_USERS=additional_username_1 additional_username_2 # VPN_ADDL_PASSWORDS=additional_password_1 additional_password_2 # Use a DNS name for the VPN server # - The DNS name must be a fully qualified domain name (FQDN) # VPN_DNS_NAME=vpn.example.com # Specify a name for the first IKEv2 client # - Use one word only, no special characters except \u0026#39;-\u0026#39; and \u0026#39;_\u0026#39; # - The default is \u0026#39;vpnclient\u0026#39; if not specified # VPN_CLIENT_NAME=your_client_name # Use alternative DNS servers # - By default, clients are set to use Google Public DNS # - Example below shows Cloudflare\u0026#39;s DNS service # VPN_DNS_SRV1=127.0.0.53 # VPN_DNS_SRV2=1.0.0.1   客户端操作  安装有 Decktop 的 ubuntu 版本 安装 VPN 客户端所需软件包：network-manager-l2tp-gnome VPN 客户端部署参考：here  客户端部署 按照以下步骤操作完成后，即可完成连接。\n Go to Settings -\u0026gt; Network -\u0026gt; VPN. Click the + button. Select Layer 2 Tunneling Protocol (L2TP). Enter anything you like in the Name field. Enter Your VPN Server IP for the Gateway. Enter Your VPN Username for the User name. Right-click the ? in the Password field, select Store the password only for all user. Enter Your VPN Password for the Password. Leave the NT Domain field blank. Click the IPsec Settings... button. Check the Enable IPsec tunnel to L2TP host checkbox. Leave the Gateway ID field blank. Enter Your VPN IPsec PSK for the Pre-shared key. Expand the Advanced section. Enter aes128-sha1-modp2048 for the Phase1 Algorithms. Enter aes128-sha1 for the Phase2 Algorithms. Click OK, then click Add to save the VPN connection information. Turn the VPN switch ON.  监听 由于客户端的连接并不能保持长久的连接，因此，我们需要一个监听脚本来进行。我写了一个简单的脚本并将其作为服务运行，可作参考。\n监听脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  # listen-vpn script #!/usr/bin/env sh dev_name=xxxx log_path=xxxx/reconnect.log ca_path=xxxx.pem echo \u0026#34;-----------------------------------------------\u0026#34; \u0026gt;\u0026gt; $log_path echo \u0026#34;$(date +%F)$(date +%T)log can write access...\u0026#34; \u0026gt;\u0026gt; $log_path echo \u0026#34;-----------------------------------------------\u0026#34; \u0026gt;\u0026gt; $log_path modifyMtu(){ nic_name=$(ifconfig | awk \u0026#39;{print $1}\u0026#39; | grep ppp | head -n 1 | awk -F \u0026#34;:\u0026#34; \u0026#39;{print $1}\u0026#39;) mtu=$(ifconfig $nic_name | awk \u0026#39;{print $4}\u0026#39; | head -n 1) vpnserver=10.4.1.13 if [ $mtu != 1500 ];then echo \u0026#34;$(date +%F)$(date +%T)mtu is low,changing to 1500...\u0026#34; \u0026gt;\u0026gt; $log_path ifconfig $nic_name mtu 1500 up \u0026gt;\u0026gt; $log_path echo \u0026#34;$(date +%F)$(date +%T)modify vpn server mtu..\u0026#34; \u0026gt;\u0026gt; $log_path ssh -i $ca_path ubuntu@$vpnserver docker exec ipsec-vpn-server ip a s $nic_name \u0026gt;\u0026gt; $log_path ssh -i $ca_path ubuntu@$vpnserver docker exec ipsec-vpn-server ifconfig $nic_name mtu 1500 up \u0026gt;\u0026gt; $log_path ssh -i $ca_path ubuntu@$vpnserver docker exec ipsec-vpn-server ip a s $nic_name \u0026gt;\u0026gt; $log_path fi } while true do ifconfig ppp0 2\u0026gt; /dev/zero if [ $? != 0 ];then echo \u0026#34;$(date +%F)$(date +%T)start vpn...\u0026#34; \u0026gt;\u0026gt; $log_path echo \u0026#34;\u0026#34; \u0026gt;\u0026gt; $log_path nmcli connection up $dev_name \u0026gt;\u0026gt; $log_path echo \u0026#34;the vpn had started...\u0026#34; \u0026gt;\u0026gt; $log_path echo \u0026#34;-----------------------------------------------\u0026#34; \u0026gt;\u0026gt; $log_path modifyMtu else modifyMtu sleep 15 fi done   服务自动运行脚本  服务存储路径：/etc/systemd/system/listen-vpn.service 使用命令sudo systemctl start/stop listen-vpn.service 启动或者停止服务 使用命令sudo systemctl enable listen-vpn.service 开启开机启动  1 2 3 4 5 6 7 8 9 10 11 12 13  # listen-vpn.service  [Unit] Description=Listen vpn NIC and modify mtu [Service] Type=simple ExecStart=/usr/bin/listen-vpn KillMode=process Restart=on-failure [Install] WantedBy=multi-user.target   ","date":"2022-06-15T16:06:19+08:00","permalink":"https://whaeon.github.io/p/network-blog01/","title":"通过 ipsec/L2TP 搭建 VPN"},{"content":"准备工作  2台虚拟机（虚拟机A 为没有任何操作系统的虚拟机，虚拟机B 为已完成安装的 openwrt 系统） 一个支持在线启动或者类似于U盘启动的系统，如 lubuntu 系统  镜像制作 扩容镜像 由于现有 openwrt 系统应当已经被使用，系统内有各种后装的软件，磁盘空间被增大。由于官方默认的磁盘空间仅有 256M，因此，需要将未安装任何软件的 openwrt 系统的基础磁盘空间扩大，以防后续拷贝现有 openwrt 系统时空间不足从而导致失败。\n操作步骤  打开使用 lubuntu 的虚拟机 A，进入 lubuntu 操作系统 下载 openwrt 系统 扩容操作可参考以下命令  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # 查看当前磁盘状况 $ lsblk # 解压文件（这里的 .gz 不是使用 tar 压缩得到的，而是通过 gzip 命令得到） $ gunzip openwrt-xxx.img.gz # 通过 dd 命令将镜像文件拷贝到磁盘中，这时，在执行 lsblk 命令，会发现磁盘设备 /dev/sda 多出来两个磁盘分区。其实就是将 img 文件的分区格式完全拷贝到了 dev/sda 设备中。在此之前，也可以执行 sudo fdisk -l openwrt-xxx.img 命令，查看 img 文件的分区格式 $ sudo dd if=openwrt-xxx-xxx-xxx.img of=/dev/sda # 这里对磁盘 dev/sda 下的 sda2 进行扩容，默认情况下 sda1 中存放的是启动所需文件。 # 分区时先记住 sda2 分区的起始 block 值，然后删除重新创建 sda2 分区，此时可以将 sda2 分区的值扩大到 1G 你想扩容的空间，且不删除 signature $ sudo fdisk /dev/sda # 扩容 sda2 的文件系统，下面的命令适用于 openwrt 文件为 ext4 格式时。其他格式是否使用该命令记得去查询一下。 $ resize2fs /dev/sda2 # 拷贝回 img 文件，完成扩容,注意这里的 bs 和 count 的大小应保持和上面扩容的的两个磁盘大小之和一致。如果不适用 bs 和 count 来规定大小，那么 dd 默认拷贝的是 sda 磁盘的全部空间大小，也包含未使用的磁盘。 $ sudo dd if=/dev/sda of=openwrt-xxx.img.gz bs=1M count=1024   拷贝当前系统内容到已扩容的 img 文件中  将上述扩容后的镜像保存到当前 openwrt 系统中 将当前系统除了存放上述扩容后的 img 文件和 proc 以及 sys 以外的文件拷贝到 img 文件中  1 2 3 4 5 6 7 8 9 10  # 随便创建的用于存放 img 文件的目录，起这个名字纯粹是为了方便后续的拷贝 $ mkdir -p /zzz/img # 将 img 文件的第二个分区挂载出来，使用 offset 的方式设置偏移量，不设置会报错。 $ sudo mount -t ext4 -o loop,offset=$((33792*512)) openwrt-xxx.img /zzz/img # 如果有其他特殊文件可以单独拷贝...拷贝完成后取消挂载即可。 $ cp -r /[a-o]* /zzz/img $ cp -r /[q-r]* /zzz/img $ cp -r /[t-y]* /zzz/img   ","date":"2022-06-15T16:03:09+08:00","permalink":"https://whaeon.github.io/p/linux-blog02/","title":"将当前 openwrt 系统打包成 img 的过程"},{"content":"获取某个 chart 的所有版本 1  helm search repo chart_name -l   删除 helm repo 中某个版本 1  curl -XDELETE http://helm.chartrepo.url/api/charts/chart-name/version   ","date":"2022-06-15T16:00:13+08:00","permalink":"https://whaeon.github.io/p/helm-blog02/","title":"helm skills"},{"content":"配置默认 StorageClass k8s 集群安装完成后，都会拥有一个默认的 storageclass。在使用 pvc 时，如果不进行特别声明，就会使用默认的storageclass，以下内容记录如何更改默认的 storageclass\n查看默认 storageclass 1 2 3 4 5  $ kubectl get storageclass NAME PROVISIONER AGE standard (default) kubernetes.io/gce-pd 1d gold kubernetes.io/gce-pd 1d   将默认 storageclass 取消 1  kubectl patch storageclass storageclass_name -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;false\u0026#34;}}}\u0026#39;   设置默认的 storageclass 1  kubectl patch storageclass storageclass_name -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39;   ","date":"2022-06-15T15:56:56+08:00","permalink":"https://whaeon.github.io/p/kubernetes-blog06/","title":"Set default StorageClasss"},{"content":"更新集群版本 这篇文章主要记录 k8s 集群的版本更新，版本更新是我们在维护集群中必然会遇到的一个问题。以下方式适用于使用 kubeadm 方式安装的集群。官方也有提供详细的更新教程，reference here\n更新前的注意内容  官方文档中标注的在开始更新前的准备\n  You need to have a kubeadm Kubernetes cluster running version 1.xx.0 or later. Swap must be disabled. The cluster should use a static control plane and etcd pods or external etcd. Make sure you read the release notes carefully. Make sure to back up any important components, such as app-level state stored in a database. kubeadm upgrade does not touch your workloads, only components internal to Kubernetes, but backups are always a best practice.   补充一些更新前的操作\n  集群中的 pod 要处于正常运行的状态，这是因为更新时会运行一个健康检查的 pod，集群如果出现异常 pod，会退出更新。 先将 kubeadm 更新到集群的下一个版本，方便更新前检查更新计划 更新集群前，做好数据备份和恢复工作，因为更新操作是不可逆的。 版本如果落后过多，更新时，最好一个版本接一个版本更新，而不要跨多个版本更新。 生产集群的工作节点更新时，记得先将工作节点节点驱逐再进行操作  更新集群版本 当所有准备都已经完成时，可以根据下面的操作进行。\n Tips: 以下更新方式应用于单 master 节点更新，多 master 节点的更新可参考：here\n  查看可用版本有哪些  1 2 3 4  sudo apt update # 该命令可以查看 kubeadm 可用的版本有哪些 sudo apt-cache madison kubeadm    将 kubeadm 更新到当前版本或者下一个版本的最后一个小版本中，否则在 kubeadm 的更新计划中还是需要先更新到最后一个版本  1  sudo apt-get install -y --allow-change-held-packages --allow-downgrades kubeadm=1.xx.xx-00    更新完 kubeadm 后，可以查看相应的更新计划  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  $ sudo kubeadm upgrade plan # 会有类似以下输出 [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [preflight] Running pre-flight checks. [upgrade] Running cluster health checks [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.21.8 [upgrade/versions] kubeadm version: v1.22.5 I1230 17:11:03.706723 9208 version.go:255] remote version is much newer: v1.23.1; falling back to: stable-1.22 [upgrade/versions] Target version: v1.22.5 [upgrade/versions] Latest version in the v1.21 series: v1.21.8 Components that must be upgraded manually after you have upgraded the control plane with \u0026#39;kubeadm upgrade apply\u0026#39;: COMPONENT CURRENT TARGET kubelet 4 x v1.21.8 v1.22.5 Upgrade to the latest stable version: COMPONENT CURRENT TARGET kube-apiserver v1.21.8 v1.22.5 kube-controller-manager v1.21.8 v1.22.5 kube-scheduler v1.21.8 v1.22.5 kube-proxy v1.21.8 v1.22.5 CoreDNS v1.8.0 v1.8.4 etcd 3.4.13-0 3.5.0-0 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.22.5 _____________________________________________________________________ The table below shows the current state of component configs as understood by this version of kubeadm. Configs that have a \u0026#34;yes\u0026#34; mark in the \u0026#34;MANUAL UPGRADE REQUIRED\u0026#34; column require manual config upgrade or resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually upgrade to is denoted in the \u0026#34;PREFERRED VERSION\u0026#34; column. API GROUP CURRENT VERSION PREFERRED VERSION MANUAL UPGRADE REQUIRED kubeproxy.config.k8s.io v1alpha1 v1alpha1 no kubelet.config.k8s.io v1beta1 v1beta1 no _____________________________________________________________________    按照上述提示操作，静待升级完成即可。  1 2 3 4 5 6 7 8  $ sudo kubeadm upgrade apply v1.22.5 # 执行成功后，会有类似以下输出(以下粘贴中省略了一部分输出) ... [upgrade/successful] SUCCESS! Your cluster was upgraded to \u0026#34;v1.22.5\u0026#34;. Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven\u0026#39;t already done so.    kubeadm 更新完成后，更新 kubelet 和 kubectl(worker 节点亦执行以下操作进行更新，注意版本保持一致)  1  sudo apt update \u0026amp;\u0026amp; sudo apt-get install -y --allow-change-held-packages --allow-downgrades kubelet=1.22.5-00 kubectl=1.22.5-00 \u0026amp;\u0026amp; sudo systemctl daemon-reload \u0026amp;\u0026amp; sudo systemctl restart kubelet   ","date":"2022-06-15T15:52:38+08:00","permalink":"https://whaeon.github.io/p/kubernetes-blog05/","title":"k8s 集群的版本更新"},{"content":"nginx ingress controller ingress 是 kubernetes 提供的一种反向代理机制，用以将集群内部的服务通过指定的域名或者端口暴露出来，从而达到统一流量入口的作用。使用 ingress 的前提是集群中要有一个 ingress controller 来监控和更新后端 service pod 的变化。\n部署方式  值得注意的是，以下两种部署方式都是在非裸金属服务器中部署\n helm 部署 如果你会使用 helm 管理工具，可以通过 helm 使用以下命令来一键部署 nginx ingress controller：\n1 2 3  helm upgrade --install ingress-nginx ingress-nginx \\  --repo https://kubernetes.github.io/ingress-nginx \\  --namespace ingress-nginx --create-namespace   kubectl 部署 除了 helm 部署之外，nginx 还提供了 kubectl 的部署方式，同样也是非常简单：\n1  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/cloud/deploy.yaml   添加 external-ip 由于 nginx ingress controller 的 service 在部署时没有通过 LoadBalance 的方式，而是通过 NodePort 的方式应用。因此，服务部署完成后，需要通过手动添加 external 的方式让 nginx ingress controller 正常工作。\nexample  获取节点 ip  1 2 3 4 5 6  $ kubectl get nodes NAME STATUS ROLES EXTERNAL-IP host-1 Ready master 203.0.113.1 host-2 Ready node 203.0.113.2 host-3 Ready node 203.0.113.3    查看 nginx ingress controller 的 service,会发现一个类型为 NodePort 的服务  1 2 3 4 5  $ kubectl get svc -n nginx-ingress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller NodePort 10.110.2.228 192.168.3.93,192.168.0.163,192.168.0.139,192.168.0.137 80:31116/TCP,443:32217/TCP 22h ingress-nginx-controller-admission ClusterIP 10.104.108.32 \u0026lt;none\u0026gt; 443/TCP 22h    编辑该服务，将节点的 external-ip 加入  1 2 3 4 5  spec: externalIPs: - 203.0.113.1 - 203.0.113.2 - 203.0.113.3    至此，nginx ingress controller 就部署完成了。  使用方式 基础使用 nginx ingress controller 部署完成后使用方式非常简单，在 spec 下添加 ingreeClassName: nginx 即可。详细解释，可以通过kubectl explain ingress.spec.ingressClassName查看。要注意的是，kubernetes 1.19 之后的版本也仅支持这种方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:creationTimestamp:\u0026#34;2020-04-07T09:28:50Z\u0026#34;generation:8labels:app:gocdchart:gocd-1.27.0component:serverheritage:Tillerrelease:gocdname:gocd-servernamespace:cdresourceVersion:\u0026#34;163526\u0026#34;uid:30f1bb4xxxxxx-xx-xx-xxspec:ingressClassName:nginxrules:- host:gocd.tthttp:paths:- backend:service:name:gocd-serverport:number:8153pathType:ImplementationSpecificstatus:loadBalancer:ingress:- ip:192.168.0.xxx  Reference 相关内容参考了：here\n","date":"2022-06-15T15:50:34+08:00","permalink":"https://whaeon.github.io/p/kubernetes-blog04/","title":"k8s 中使用 nginx 作为 ingress controller"},{"content":"CKA Coupon DCUBEOFFER\n","date":"2022-06-15T15:48:27+08:00","permalink":"https://whaeon.github.io/p/memo-blog03/","title":"CKA 英文版报名优惠券"},{"content":"背景 k8s 安装过程中，由于想要查看集群中 pod 的 cpu 和 memory 的使用情况，因此需要 metrics server 的支持。\n metrics server 能够收集 kubelet 中的各种资源指标，并将其 expose 到 apiserver 中，以便于实现对集群资源的监控和管理。\n 安装 通过 here 可以了解到详细的 metrics server 的资料。 metrics server 可以通过官方提供的在线的 yaml 文件的形式直接安装：\n1  kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml   BUG 安装完成后，发现 pod 迟迟无法启动，查看 pod 的 describe 时，出现 error： x509: cannot validate certificate for 172.16.52.132 because it doesn't contain any IP SANs\nTroubleshooting 出现该问题的原因主要是： 默认情况下，kubelet 的证书是通过 kubeadm 自签名获得，在引入的一些外部服务中签名不被承认所导致的\nSolutions  方法一 在安装 metrics server 前，将官方提供的 yaml 文件下载下来后，添加参数 --kubelet-insecure-tls 即可。大致位置，可以参考以下内容的最后一行。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  apiVersion:apps/v1kind:Deploymentmetadata:labels:k8s-app:metrics-servername:metrics-servernamespace:kube-systemspec:selector:matchLabels:k8s-app:metrics-serverstrategy:rollingUpdate:maxUnavailable:0template:metadata:labels:k8s-app:metrics-serverspec:containers:- args:- --cert-dir=/tmp- --secure-port=4443- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname- --kubelet-use-node-status-port- --metric-resolution=15s- --kubelet-insecure-tls......   方法二 通过获取正确的证书服务来解决 编辑 kube-system 命名空间下的 kubelet-config-1.23 文件，加入 serverTLSBootstrap: true,然后重启每一个节点的 kubelet 服务即可。  Reference github issue offical docs\n","date":"2022-06-15T15:44:23+08:00","permalink":"https://whaeon.github.io/p/kubernetes-blog03/","title":"kubernetes 安装 metrics server 时，出现 SANs 的问题"},{"content":"简介 使用虚拟机创建的 linux 过程中，经常会遇到由于初始磁盘空间分配较小，后续磁盘可能会被 docker 镜像之类的或者大文件占满。此时，如果想要扩充磁盘空间就可以通过创建新的磁盘分区，然后将该磁盘分区作为 pv 加入到 vg 中。但如果不想要新建一个 pv，可以通过下面的方式进行扩容\n应用  虚拟机关机，扩容磁盘空间，然后使用 fdisk 工具扩容 根磁盘   example\n 1 2 3 4 5 6 7 8 9 10 11 12 13  sudo fdisk /dev/sda \u0026lt;\u0026lt; EOF p d n N p w EOF # 注意上面中的空行，空行是为了跳过 fdisk 的一些输入内容，如果复制使用出错，可以按照 EOF 后的内容一步步操作。    重新调整 pv 的大小  1 2 3 4 5  # 查看当前主机的 pv 状态 sudo pvs # 重新调整已扩充后的磁盘分区在 pv 中的大小 sudo pvresize /dev/sda    直接扩容 lv，然后 resize 一次 lv 的文件系统即可。  1 2 3 4 5 6 7 8 9 10 11  # 可以看到，vg 在 pv 扩容完成后直接就多出来扩容的空间了 sudo vgs # 扩容 lv sudo lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv # 查看 lv 的文件系统类型,基本上都为 ext4 类型 sudo blkid /dev/mapper/ubuntu--vg-ubuntu--lv # resize2fs 扩容 ext4 类型的 lv sudo resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv   ","date":"2022-06-15T15:40:36+08:00","permalink":"https://whaeon.github.io/p/linux-blog01/","title":"虚拟机磁盘根目录扩容"},{"content":"Note 一  k8s 中的 static pod 是通过在pod名字后使用连字符加节点名称的方式来表示的\u0026hellip; static pod 的源文件应存放在在 /etc/kubernetes/manifests/ ， pod 启动后，会自动添加后缀来表示。 就好像这样： multi-container-playground-cluster1-worker1  两  容器的 affinity 中，关于 topologykey 的理解：当 topologykey 所标记的值一致时，kubernets 则会认为这些相关的 pods/nodes 属于同一个域中。  三  通过 kubeadm 命令检查集群证书过期时间：kubeadm certs check-expiration | grep apiserver 或者通过解码集群证书进行验证：openssh x509 -noout -text -in /etc/kubernetes/pki/apiserver.crt  四  kubernetes 在节点资源不足时，会优先删除实际使用资源大于 request 中需要的资源的 pod，而那些没有添加 request 定义的 pod 会被默认为资源使用超限。参考：here  ","date":"2022-06-15T15:25:20+08:00","permalink":"https://whaeon.github.io/p/kubernetes-blog02/","title":"kubernetes 零散笔记"},{"content":"现象 在 k8s cluster 创建 pvc 时，遇到了一个坑，pvc 在创建完成后一直处于 pending 状态。在进故障排查时，发现 provisioner 的 container 日志中出现了以下内容： unexpected error getting claim reference: selfLink was empty, can't make reference 一番查找后，发现 github 上已有关于该问题的解决方案和相关介绍。 出现该问题的原因是由于 kubernetes 在 1.20.0 以后的版本中将 selfLink 移除而导致。相关文章可以参考以下链接：\n Deprecate and remove SelfLink \n Troubleshooting 修改 api-server 的配置文件，默认路径为：/etc/kubernetes/manifests/kube-apiserver.yaml 在文件中找到如下内容，并插入 - --feature-gates=RemoveSelfLink=false 参数。\n1 2 3 4  spec:containers:- command:- kube-apiserver  kubelet 会监听 api-server 文件，如果有改动，kubelet 会自动删除当前运行的 api-server 的 pod, 然后重新创建一个新的 api-server pod. 如果有多个 Kubernetes Master 节点，则需要在每一个 Master 节点上都修改该文件，并使各节点上的参数保持一致。 如果 api-server 自动重启失败，可尝试通过命令kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml手动启动。\nReference Using Kubernetes v1.20.0, getting \u0026ldquo;unexpected error getting claim reference: selfLink was empty, can\u0026rsquo;t make reference\u0026rdquo; 附两个关于什么是 selfLink 的说明，目前仍然不是很理解。 Question: what\u0026rsquo;s the functionality of SelfLink in ObjectMeta? ObjectMeta\n","date":"2022-06-15T15:11:21+08:00","permalink":"https://whaeon.github.io/p/kubernetes-blog01/","title":"kubernetes error: selfLink was empty"},{"content":" 摄像头测试  1 2 3 4  $ sudo apt install cheese # 测试 $ cheese    蓝牙功能  1  $ sudo service bluetooth start   ","date":"2022-06-15T15:09:09+08:00","permalink":"https://whaeon.github.io/p/memo-blog02/","title":"kali 常用软件安装"},{"content":"安装微信乱码 cutefish OS 英文版系统使用官方的 store 安装完成微信后，出现乱码问题。\n故障分析 网上相关联的资料后，发现是由于 wine 缺少字体导致的。cutefish 中微信安装完成后，字体的默认位置在：~/.cutefish-wine/wechat/drive_c/windows/Fonts\n解决办法 处理方式很简单，下载 微软雅黑 字体，并将字体放到上面的目录下，重启微信即可\n终端管理 cutefish OS 默认安装的终端不能使用鼠标中键进行粘贴，这对于习惯使用中键的我来说简直是折磨，于是我很干脆的换成了 yakuake 终端，下拉式终端，使用体验简直绝了。 下一步就是安装 zsh，然后进行美化了，使用了常见的 oh-my-zsh 进行配置，然后又加入了 zsh-autosuggestions 插件。启动插件的方式还是要记录一下：\n 打开 .zshrc 配置文件，其路径默认是：~/.zshrc 找到 plugins 所在，然后加入 zsh-autosuggestions 插件，配置完成后大概长下面这样  1 2 3  ... plugins=(git zsh-autosuggestions) ...    重新加载配置文件即可（source ~/.zshrc）。  安装部分第三方软件包图标不显示  custfish 桌面图标集成到了 /usr/share/icons/hicolor 目录内，在此目录内的 icon 文件，均可以在配置 Desktop 配置文件内的 Icon 属性时忽略路径以及后缀名。 dock 的配置文件路径在 ~/.config/cutefishos/dock_pinned.conf  解决步骤  拷贝第三方应用程序的 .desktop 文件到 /usr/share/applications/ 目录下 拷贝第三方应用程序配置文件中 Icon 属性对应路径下的图标文件到 /usr/share/icons/hicolor 对应的目录下 修改 /usr/share/applications/yourapp.desktop 文件内的 Icon 值为第二步操作的文件名(如：Icon=/path/icon.png 可改为 Icon=icon) 打开 launcher \u0026ndash;\u0026gt; 找到对应的第三方应用程序 \u0026ndash;\u0026gt; 右击 \u0026ndash;\u0026gt; 发送到 dock 修改 dock 配置文件中对应的第三方应用程序中 Icon 属性值。 重启操作系统。  ","date":"2022-06-15T15:01:50+08:00","permalink":"https://whaeon.github.io/p/memo-blog01/","title":"cutefish OS 使用小技巧"},{"content":"简介 网站的使用少不了域名，而域名在使用过程中少不了 SSL 证书的使用。绝大多数域名机构对于免费的证书都有着一定的限制条件，而付费的证书往往价格高昂，对于中小企业来说无疑是一笔很大的支出。本文主要用来记录如何申请免费的证书以及相关的一些自动化操作。\nLet\u0026rsquo;s Encrypt 免费证书 Let\u0026rsquo;s Encrypt 是一家免费的证书颁发机构，其目的在于推广 https 的使用。\n通过 certbot 申请免费证书 certbot 是 letsencrypt 提供的一个用于命令行的获取证书的客户端，安装方式也已经被集成到各种系统软件仓库之中，可以直接安装。\n1 2 3 4 5 6 7 8  # ubuntu $ sudo apt install -y certbot # CentOS $ sudo yum install -y certbot # MacOS $ sudo brew install certbot   安装完成后，即可通过 certbot 来对域名申请免费的证书。\n申请域名的几种方式  申请域名时需要通过一定的方式来验证域名所有者\n  通过 dns 域名解析的方式来申请证书（推荐）  1 2 3 4 5 6 7 8  # 注意替换下面 email 和 domain 位置为自己申请的域名地址。 # 执行完成后，将输出中的 dns 解析记录添加到对应域名托管商中。 $ sudo certbot --manual --preferred-challenges dns certonly \u0026lt;\u0026lt; EOF email A domain Y EOF    通过在网站根目录下添加文件来进行验证。多数情况下我们可能是为某个内网服务申请证书，而 letsencrypt 的服务器可能没办法访问到该主机，因此，可能会申请失败。  let\u0026rsquo;s encrypt 生成的证书详情  cert.pem contains the certificate - public key and metatdata (issuer, serialnumber, subject, SAN, attributes and extensions). privkey.pem contains the private key of your certificate. chain.pem contains your certificate and its issuer - there could be more instances in the chain like Root CA -\u0026gt; Sub CA -\u0026gt; your cert. fullchain.pem contains your certificate and all cas up to the root ca.  自动续期 由于 letsencrypt 申请的证书使用期只有3个月，经常人工需续期太累了。\n通过 certboot renew 命令进行续期。\n1  certbot renew --force-renew --manual-auth-hook renewdns.sh   --force-renew参数代表强制进行SSL证书续期 --manual-auth-hook参数代表自定义验证脚本，这里脚本的内容就是主机名为_acme-challenge的TXT记录。 至于这里的自定义脚本怎么写，这个就需要看DNS服务商了，每家DNS服务商都不一样的。\n","date":"2022-06-15T14:51:49+08:00","permalink":"https://whaeon.github.io/p/letsencrypt-blog01/","title":"使用 Let's Encrypt 申请免费证书"},{"content":"故障现象 使用 helm chart 安装或者升级服务时，出现异常中断后重新运行 upgrade 就会容易(如果有chart包中的单个服务进行过rollout操作，可能也会出现相同问题，但未测试过)出现以下报错： Error: UPGRADE FAILED: another operation (install/upgrade/rollback) is in progress\n故障处理  通过 helm history -n namespace chartName or kubectl get secrets -n chartNamespace 方式，查看最近一次 chart 包的升级或者安装结果 然后将其删除重新运行command或者pipeline即可。  1 2  kubectl get secret -n \u0026lt;namespace\u0026gt; kubectl delete secret sh.helm.release.v1.\u0026lt;RELEASE_NAME\u0026gt;.v\u0026lt;LATEST_REVISION\u0026gt;   Reference  kubernetes-helm-stuck-with-an-update-in-progress github helm issue  ","date":"2022-06-15T14:04:48+08:00","permalink":"https://whaeon.github.io/p/helm-blog01/","title":"Helm Deploy Error"},{"content":"设置vim的tab键字符宽度 设置方式，设置全局字符宽度 使用 vim 打开 /etc/vim/vimrc ,然后加入以下内容：\n1 2  set ts=4 set noexpandtab   expandtab 代表把 tab 缩进用空格来替代\n设置自动缩进 设置自动缩进：即每行的缩进值与上一行相等；使用 noautoindent 可以取消设置\n1  set autoindent   设置换行后的自动缩进值 编写 yaml 格式的文件，每次换行默认为 8 个缩进，范围太大，我这强迫症看的是真滴不舒服。\n1  set shiftwidth=4   ","date":"2022-06-15T13:41:18+08:00","permalink":"https://whaeon.github.io/p/vim-blog01/","title":"Vim Config"}]